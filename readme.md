
# Comment Toxicity Model

This project is a Deep Learning-based model developed using Python to detect and classify toxic comments. The model identifies various levels of toxicity, including whether the comment is obscene, insulting, or contains hate speech. 

## Features

- **Toxicity Detection:** Determines if a comment is toxic or not.
- **Level Classification:** Classifies the severity of the toxicity into categories like obscene, threat, insult, etc.
- **Interactive Interface:** Users can input comments and get instant feedback on the level of toxicity.

## Example Outputs

Here are some example outputs from the model:

### 1. Toxic Comment
![Toxic Comment](./Screenshot_2024-09-08_185853.png)

### 2. Severe Toxic Comment
![Severe Toxic Comment](./Screenshot_2024-09-08_185959.png)

### 3. Non-Toxic Comment
![Non-Toxic Comment](./Screenshot_2024-09-08_190058.png)


## Libraries Used

- **TensorFlow/Keras**
- **Scikit-learn**
- **Flask (for the web interface)**
- **Pandas**

## Future Enhancements

- **Improve model accuracy:** Tuning the model to better handle edge cases.
- **Add more features:** Implement more detailed categories for toxicity and additional levels of classification.

